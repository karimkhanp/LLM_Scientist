🔖 Bookmark this and follow the curriculum:

𝟭: 𝗟𝗟𝗠 𝗔𝗿𝗰𝗵𝗶𝘁𝗲𝗰𝘁𝘂𝗿𝗲 🏗️
- The Illustrated Transformer: https://lnkd.in/e7NihY_K
- Visual intro to Transformers: https://lnkd.in/eJTF9VG8
- nanoGPT by Andrej Karpathy: https://lnkd.in/e5z4r3qK

𝟮: 𝗕𝘂𝗶𝗹𝗱𝗶𝗻𝗴 𝗮𝗻 𝗜𝗻𝘀𝘁𝗿𝘂𝗰𝘁𝗶𝗼𝗻 𝗗𝗮𝘁𝗮𝘀𝗲𝘁 📚
- Preparing a Dataset for Instruction Tuning: https://lnkd.in/esR7d7bV
- GPT 3.5 for News Classification: https://lnkd.in/e9kFWwGs
- Chat Template by Matthew Carrigan: https://lnkd.in/eJWGc93U

𝟯: 𝗣𝗿𝗲-𝘁𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗠𝗼𝗱𝗲𝗹𝘀 🔄
- LLMDataHub: https://lnkd.in/eerWkRxD
- TinyLlama by Zhang et al: https://lnkd.in/eNSS6S9p
- BLOOM by BigScience: https://lnkd.in/eFJpt2yQ

𝟰: 𝗦𝘂𝗽𝗲𝗿𝘃𝗶𝘀𝗲𝗱 𝗙𝗶𝗻𝗲-𝗧𝘂𝗻𝗶𝗻𝗴 🎯
- Fine-Tune Your Own Llama 2 Model: https://lnkd.in/eFgnKN8S
- LoRA Insights by Sebastian Raschka: https://lnkd.in/e4n7NWSc
- A Beginner's Guide to LLM Fine-Tuning: https://lnkd.in/eMX5nY8h

𝟱: 𝗣𝗿𝗲𝗳𝗲𝗿𝗲𝗻𝗰𝗲 𝗔𝗹𝗶𝗴𝗻𝗺𝗲𝗻𝘁 ⚖️
- Distilabel by Argilla (tool to create your own datasets): https://lnkd.in/ev-zCeYT
- An Introduction to Training LLMs using RLHF: https://lnkd.in/e54pHTxH
- Fine-tune Mistral-7b with DPO: https://lnkd.in/eWuXTp_m

𝟲: 𝗘𝘃𝗮𝗹𝘂𝗮𝘁𝗶𝗼𝗻 ✓
- Perplexity of Fixed-Length Models: https://lnkd.in/epnrGVpd
- BLEU at Your Own Risk by Rachael Tatman: https://lnkd.in/efpdMNm3
- A survey on evaluation of LLMs: https://lnkd.in/efP2xU7w

𝟳: 𝗤𝘂𝗮𝗻𝘁𝗶𝘇𝗮𝘁𝗶𝗼𝗻 📏
- Introduction to Quantization: 
https://lnkd.in/ebYTDnxt
- Quantize Llama Models with llama.cpp: https://lnkd.in/eQGkWxq6
- 4-bit LLM Quantization with GPTQ: https://lnkd.in/eyDjVxGd

𝟴: 𝗡𝗲𝘄 𝗧𝗿𝗲𝗻𝗱𝘀 🌟
- Extending the RoPE by EleutherAI: https://lnkd.in/eGCZ-nsn
- Understanding YaRN by Rajat Chawla: https://lnkd.in/eQ-nRxVD
- Large Multimodal Models by Chip Huyen: https://lnkd.in/e9xQmFKh
