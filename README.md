ğŸ”– Bookmark this and follow the curriculum:

ğŸ­: ğ—Ÿğ—Ÿğ—  ğ—”ğ—¿ğ—°ğ—µğ—¶ğ˜ğ—²ğ—°ğ˜ğ˜‚ğ—¿ğ—² ğŸ—ï¸
- The Illustrated Transformer: https://lnkd.in/e7NihY_K
- Visual intro to Transformers: https://lnkd.in/eJTF9VG8
- nanoGPT by Andrej Karpathy: https://lnkd.in/e5z4r3qK

ğŸ®: ğ—•ğ˜‚ğ—¶ğ—¹ğ—±ğ—¶ğ—»ğ—´ ğ—®ğ—» ğ—œğ—»ğ˜€ğ˜ğ—¿ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ——ğ—®ğ˜ğ—®ğ˜€ğ—²ğ˜ ğŸ“š
- Preparing a Dataset for Instruction Tuning: https://lnkd.in/esR7d7bV
- GPT 3.5 for News Classification: https://lnkd.in/e9kFWwGs
- Chat Template by Matthew Carrigan: https://lnkd.in/eJWGc93U

ğŸ¯: ğ—£ğ—¿ğ—²-ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ— ğ—¼ğ—±ğ—²ğ—¹ğ˜€ ğŸ”„
- LLMDataHub: https://lnkd.in/eerWkRxD
- TinyLlama by Zhang et al: https://lnkd.in/eNSS6S9p
- BLOOM by BigScience: https://lnkd.in/eFJpt2yQ

ğŸ°: ğ—¦ğ˜‚ğ—½ğ—²ğ—¿ğ˜ƒğ—¶ğ˜€ğ—²ğ—± ğ—™ğ—¶ğ—»ğ—²-ğ—§ğ˜‚ğ—»ğ—¶ğ—»ğ—´ ğŸ¯
- Fine-Tune Your Own Llama 2 Model: https://lnkd.in/eFgnKN8S
- LoRA Insights by Sebastian Raschka: https://lnkd.in/e4n7NWSc
- A Beginner's Guide to LLM Fine-Tuning: https://lnkd.in/eMX5nY8h

ğŸ±: ğ—£ğ—¿ğ—²ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² ğ—”ğ—¹ğ—¶ğ—´ğ—»ğ—ºğ—²ğ—»ğ˜ âš–ï¸
- Distilabel by Argilla (tool to create your own datasets): https://lnkd.in/ev-zCeYT
- An Introduction to Training LLMs using RLHF: https://lnkd.in/e54pHTxH
- Fine-tune Mistral-7b with DPO: https://lnkd.in/eWuXTp_m

ğŸ²: ğ—˜ğ˜ƒğ—®ğ—¹ğ˜‚ğ—®ğ˜ğ—¶ğ—¼ğ—» âœ“
- Perplexity of Fixed-Length Models: https://lnkd.in/epnrGVpd
- BLEU at Your Own Risk by Rachael Tatman: https://lnkd.in/efpdMNm3
- A survey on evaluation of LLMs: https://lnkd.in/efP2xU7w

ğŸ³: ğ—¤ğ˜‚ğ—®ğ—»ğ˜ğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—» ğŸ“
- Introduction to Quantization: 
https://lnkd.in/ebYTDnxt
- Quantize Llama Models with llama.cpp: https://lnkd.in/eQGkWxq6
- 4-bit LLM Quantization with GPTQ: https://lnkd.in/eyDjVxGd

ğŸ´: ğ—¡ğ—²ğ˜„ ğ—§ğ—¿ğ—²ğ—»ğ—±ğ˜€ ğŸŒŸ
- Extending the RoPE by EleutherAI: https://lnkd.in/eGCZ-nsn
- Understanding YaRN by Rajat Chawla: https://lnkd.in/eQ-nRxVD
- Large Multimodal Models by Chip Huyen: https://lnkd.in/e9xQmFKh
